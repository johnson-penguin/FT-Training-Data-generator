{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… JSON saved to: /home/aiml/johnson/Scenario/Scenario_Latest_for_cu_testing/input_data/conf/68_cu_gnb_local_s_if_name_segments.json\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "import json\n",
        "\n",
        "def cu_conf_to_json(conf_path, output_json_path):\n",
        "    with open(conf_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    result = []\n",
        "    assign_pattern = re.compile(r'^\\s*([A-Za-z0-9_]+)\\s*=\\s*(.+?);\\s*(#.*)?$')\n",
        "\n",
        "    for line in lines:\n",
        "        stripped = line.strip()\n",
        "        if stripped.startswith(\"#\") or not stripped:\n",
        "            continue  # Skip comment or empty lines\n",
        "\n",
        "        match = assign_pattern.match(stripped)\n",
        "        if match:\n",
        "            key = match.group(1)\n",
        "            value = match.group(2).strip()\n",
        "            full_content = f\"{key} = {value};\"\n",
        "            result.append({\n",
        "                \"label\": key,\n",
        "                \"content\": full_content\n",
        "            })\n",
        "\n",
        "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(result, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"âœ… JSON saved to: {output_json_path}\")\n",
        "\n",
        "base_dir = Path().resolve()\n",
        "cu_input_index = \"68_cu_gnb_local_s_if_name\"\n",
        "\n",
        "# æŒ‡å®š log æª”æ¡ˆ\n",
        "# du_log_file = \"/home/aiml/johnson/Scenario/Scenario_For_testing/DU/log/du.log\"\n",
        "# ru_log_file = \"/home/aiml/johnson/Scenario/Scenario_For_testing/RU/log/RU.log\"\n",
        "\n",
        "# pcap_path = \"/home/aiml/johnson/Scenario/Scenario_For_testing/FH/fh.pcap\"\n",
        "debug_yaml_path = base_dir / \"reference_data\" / \"debug.yaml\"\n",
        "reference_context_path = base_dir / \"reference_data\" / \"reference_config.txt\"\n",
        "# -----------------------\n",
        "\n",
        "\n",
        "cu_log_file = base_dir / \"input_data\"/ \"log\" / f\"{cu_input_index}_log.txt\"\n",
        "current_cu_config_path = base_dir / \"input_data\" / \"conf\"/ f\"{cu_input_index}.conf\"\n",
        "current_cu_config_json_path = current_cu_config_path.with_name(\n",
        "    current_cu_config_path.stem + \"_segments.json\"\n",
        ")\n",
        "cu_conf_to_json(current_cu_config_path, current_cu_config_json_path)\n",
        "\n",
        "rag_after_cu_conf_path = base_dir / \"output_data\" / f\"{cu_input_index}_modification.conf\"\n",
        "rag_after_cu_json_path = base_dir / \"output_data\" / f\"{cu_input_index}_modification.conf.segments.json\"\n",
        "diff_log_path = base_dir / \"output_data\" / f\"{cu_input_index}_diff.log\"\n",
        "\n",
        "\n",
        "\n",
        "# current_du_config_path=\"/home/aiml/johnson/Scenario/Scenario_For_testing/DU/conf/du.conf\"\n",
        "# rag_after_du_conf_path=\"/home/aiml/johnson/Scenario/Scenario_For_testing/DU/conf/Scenario_For_testing_du_modification_1.conf\"\n",
        "# rag_after_du_json_path=\"/home/aiml/johnson/Scenario/Scenario_For_testing/DU/conf/Scenario_For_testing_du_modification_1.conf.segments.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "from pathlib import Path\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "import difflib\n",
        "\n",
        "\n",
        "def old_parse_llm_response(response_text):\n",
        "    # Try markdown-wrapped JSON first\n",
        "    match = re.search(r\"```json\\s*(\\[\\s*{.*?}\\s*\\])\\s*```\", response_text, re.DOTALL)\n",
        "    if not match:\n",
        "        # Fallback: Try raw array in text\n",
        "        match = re.search(r\"(\\[\\s*{.*?}\\s*\\])\", response_text, re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group(1))\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(\"âŒ JSON decode error:\", e)\n",
        "            return []\n",
        "    else:\n",
        "        print(\"âš ï¸ No JSON block found in LLM response.\")\n",
        "        return []\n",
        "\n",
        "def parse_llm_response(response_text):\n",
        "    \"\"\"\n",
        "    Parse LLM JSON array from response text, clean formatting issues and return as Python list.\n",
        "    \"\"\"\n",
        "    # 1. å»é™¤ ```json åŒ…è£¹\n",
        "    response_text = re.sub(r\"```json\\s*\", \"\", response_text)\n",
        "    response_text = re.sub(r\"```\", \"\", response_text)\n",
        "\n",
        "    # 2. å˜—è©¦æ“·å– JSON é™£åˆ—\n",
        "    match = re.search(r\"(\\[\\s*{.*?}\\s*\\])\", response_text, re.DOTALL)\n",
        "    if not match:\n",
        "        print(\"âš ï¸ No JSON block found in LLM response.\")\n",
        "        return []\n",
        "\n",
        "    raw_json = match.group(1)\n",
        "\n",
        "    # 3. ä¿®æ­£å¸¸è¦‹éŒ¯èª¤ï¼šlike `tr_s_preference = ;` â†’ åŠ ä¸Šå¼•è™Ÿ\n",
        "    raw_json = re.sub(r'=\\s*;', '= \"\";', raw_json)\n",
        "\n",
        "    # 4. ä¿®æ­£ content æ¬„ä½ä¸­éŒ¯èª¤ä½¿ç”¨å–®å¼•è™Ÿçš„æƒ…æ³\n",
        "    raw_json = re.sub(r'\"content\":\\s*\\'(.*?)\\'', lambda m: '\"content\": \"{}\"'.format(m.group(1).replace('\"', '\\\\\"')), raw_json)\n",
        "\n",
        "    # 5. å˜—è©¦è§£æ JSON\n",
        "    try:\n",
        "        return json.loads(raw_json)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"âŒ JSON decode error:\", e)\n",
        "        print(\"ğŸ§ª Raw JSON:\\n\", raw_json)\n",
        "        return []\n",
        "    \n",
        "def process_config_type(config_type, suggestions, current_path, modified_path, diff_log_path):\n",
        "    sft_data = []\n",
        "    if not suggestions:\n",
        "        safe_print(f\"ğŸ“„ No LLM suggestions for {config_type}.\")\n",
        "        return\n",
        "\n",
        "    content, modified_labels, change_log = apply_llm_suggestions(\n",
        "        conf_path=current_path,\n",
        "        output_path=modified_path,\n",
        "        llm_suggestions=suggestions,\n",
        "        config_type=config_type\n",
        "    )\n",
        "\n",
        "    save_modified_config(content, modified_path, config_type)\n",
        "    compare_conf_files(current_path, modified_path, diff_log_path)\n",
        "    save_sft_data(change_log, modified_path, config_type, suggestions, current_path)\n",
        "\n",
        "def odd_save_modified_config(content, output_path, config_type):\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(content)\n",
        "    safe_print(f\"âœ… [{config_type}] Updated file: {output_path}\")\n",
        "\n",
        "\n",
        "    json_path = os.path.splitext(output_path)[0] + f\"_{config_type}_sft.json\"\n",
        "    with open(json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(sft_data, f, indent=2, ensure_ascii=False)\n",
        "    safe_print(f\"\\nğŸ“ [{config_type}] SFT data saved to: {json_path}\")\n",
        "\n",
        "def save_modified_config(content, output_path, config_type):\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(content)\n",
        "    safe_print(f\"âœ… [{config_type}] Updated file: {output_path}\")\n",
        "\n",
        "def compare_conf_files(file1, file2, output_path):\n",
        "    file1 = str(file1)\n",
        "    file2 = str(file2)\n",
        "    output_path = str(output_path)\n",
        "\n",
        "    with open(file1, 'r', encoding='utf-8') as f1, open(file2, 'r', encoding='utf-8') as f2:\n",
        "        lines1 = f1.readlines()\n",
        "        lines2 = f2.readlines()\n",
        "\n",
        "    diff = difflib.unified_diff(\n",
        "        lines1, lines2,\n",
        "        fromfile=file1,\n",
        "        tofile=file2,\n",
        "        lineterm=''\n",
        "    )\n",
        "\n",
        "    modified_lines = [\n",
        "        line for line in diff\n",
        "        if (line.startswith('+') or line.startswith('-')) and not line.startswith('+++') and not line.startswith('---')\n",
        "    ]\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(modified_lines))\n",
        "\n",
        "    print(f\"âœ… Diff saved to: {output_path}\")\n",
        "\n",
        "def old_save_sft_data(change_log, output_path, config_type, suggestions, current_path):\n",
        "    reason_map = {s[\"label\"]: s.get(\"reference_reason\", \"\") for s in suggestions}\n",
        "    for label, before, after, model_reason in change_log:\n",
        "        sft_data.append({\n",
        "            \"label\": label,\n",
        "            \"before\": before,\n",
        "            \"after\": after,\n",
        "            \"model_reason\": model_reason,\n",
        "            \"reference_reason\": reason_map.get(label, \"\"),\n",
        "            \"config_type\": config_type,\n",
        "            \"source_file\": os.path.basename(current_path)\n",
        "        })\n",
        "\n",
        "def save_sft_data(change_log, output_path, config_type, suggestions, current_path):\n",
        "    sft_data = []  # âœ… è£œä¸Šé€™è¡Œ\n",
        "    reason_map = {s[\"label\"]: s.get(\"reference_reason\", \"\") for s in suggestions}\n",
        "    \n",
        "    for label, before, after, model_reason in change_log:\n",
        "        sft_data.append({\n",
        "            \"label\": label,\n",
        "            \"before\": before,\n",
        "            \"after\": after,\n",
        "            \"model_reason\": model_reason,\n",
        "            \"reference_reason\": reason_map.get(label, \"\"),\n",
        "            \"config_type\": config_type,\n",
        "            \"source_file\": os.path.basename(current_path)\n",
        "        })\n",
        "\n",
        "    # âœ… æ­£ç¢ºå„²å­˜ JSON çš„é‚è¼¯ï¼ˆè‡ªå‹•è½‰æˆ _CU_sft.jsonï¼‰\n",
        "    json_path = os.path.splitext(output_path)[0] + f\"_{config_type}_sft.json\"\n",
        "    with open(json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(sft_data, f, indent=2, ensure_ascii=False)\n",
        "    safe_print(f\"\\nğŸ“ [{config_type}] SFT data saved to: {json_path}\")\n",
        "\n",
        "def safe_print(text):\n",
        "    try:\n",
        "        print(text.encode('utf-8', 'replace').decode('utf-8'))\n",
        "    except Exception:\n",
        "        print(\"[Output error suppressed]\")\n",
        "\n",
        "def apply_llm_suggestions(conf_path, output_path, llm_suggestions, config_type):\n",
        "    with open(conf_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        content = f.read()\n",
        "\n",
        "    modified_labels = []\n",
        "    change_log = []\n",
        "\n",
        "    for suggestion in llm_suggestions:\n",
        "        label = suggestion[\"label\"]\n",
        "        replacement = suggestion[\"content\"]\n",
        "        model_reason = suggestion.get(\"model_reason\", \"\")\n",
        "\n",
        "        pattern = rf\"{label}\\s*=\\s*.*?;\"\n",
        "        match = re.search(pattern, content, flags=re.DOTALL)\n",
        "\n",
        "        if match:\n",
        "            original_line = match.group(0).strip()\n",
        "            if original_line != replacement.strip():\n",
        "                content = re.sub(pattern, replacement, content, flags=re.DOTALL)\n",
        "                modified_labels.append(label)\n",
        "                change_log.append((label, original_line, replacement.strip(), model_reason))\n",
        "            else:\n",
        "                safe_print(f\"â„¹ï¸ [{config_type}] {label} already matches suggested value.\")\n",
        "        else:\n",
        "            safe_print(f\"âš ï¸ [{config_type}] No matching setting found: {label}\")\n",
        "\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(content)\n",
        "\n",
        "    safe_print(f\"âœ… [{config_type}] Updated file: {output_path}\")\n",
        "\n",
        "    if modified_labels:\n",
        "        safe_print(f\"ğŸ› ï¸ [{config_type}] Modified parameters:\")\n",
        "        for label in modified_labels:\n",
        "            safe_print(f\" - {label}\")\n",
        "    else:\n",
        "        safe_print(f\"ğŸ“­ [{config_type}] No parameters were modified\")\n",
        "\n",
        "    return content, modified_labels, change_log\n",
        "\n",
        "def split_suggestions_by_target(llm_suggestions):\n",
        "    cu_suggestions = []\n",
        "    du_suggestions = []\n",
        "    for s in llm_suggestions:\n",
        "        if s.get(\"target\") == \"CU\":\n",
        "            cu_suggestions.append(s)\n",
        "        elif s.get(\"target\") == \"DU\":\n",
        "            du_suggestions.append(s)\n",
        "    return cu_suggestions, du_suggestions\n",
        "\n",
        "def clean_text(s):\n",
        "    \"\"\"å»é™¤ANSIæ§åˆ¶å­—å…ƒ + ç§»é™¤å¼•è™Ÿ + å»é™¤å¤šé¤˜ç©ºæ ¼\"\"\"\n",
        "    ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n",
        "    s = ansi_escape.sub('', s)\n",
        "    s = s.replace(\"'\", \"\").replace('\"', \"\")\n",
        "    s = s.strip()\n",
        "    return s\n",
        "\n",
        "def extract_config_key(line: str) -> str:\n",
        "    # ç§»é™¤ BOM, éå¯åˆ—å°ç©ºç™½ç­‰ç‰¹æ®Šç¬¦è™Ÿ\n",
        "    line = line.encode(\"ascii\", errors=\"ignore\").decode().strip()\n",
        "    line = line.split(';')[0]  # ç§»é™¤è¨»è§£æˆ–è¡Œå°¾åˆ†è™Ÿ\n",
        "    match = re.match(r\"(\\w+)\\s*=\", line)\n",
        "    return match.group(1) if match else \"\"\n",
        "\n",
        "def find_case_by_related_config(config_key, vectordb, top_k=3):\n",
        "    results = vectordb.similarity_search(config_key, k=top_k)\n",
        "    return next(\n",
        "        (doc for doc in results if config_key in doc.metadata.get('related_config', [])),\n",
        "        None\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Debug embedding å»ºç«‹å®Œæˆä¸¦å·²å„²å­˜\n",
            "ğŸ“¦ ç¸½ç­†æ•¸ï¼š 70\n",
            "\n",
            "--- Entry 1 ---\n",
            "Document ID: 7dfa0368-da95-48b1-a61f-95887eac5530\n",
            "Document Text: Stage: cu_init_success\n",
            "Symptom: CU initialization success\n",
            "Log: Initialization complete without error\n",
            "Related Config: \n",
            "Notes: The CU started successfully and no issues were detected in the configuration file.\n",
            "All required parameters are correctly set.\n",
            "No changes are required at this stage.\n",
            "\n",
            "Metadata: {'notes': 'The CU started successfully and no issues were detected in the configuration file.\\nAll required parameters are correctly set.\\nNo changes are required at this stage.\\n', 'related_config': '', 'stage': 'cu_init_success', 'symptom': 'CU initialization success'}\n",
            "\n",
            "--- Last Entry ---\n",
            "Document ID: 4aa0a09f-97b4-442a-a9b2-a0649da4eac5\n",
            "Document Text: Stage: cu_config_parsing\n",
            "Symptom: CU failed to start due to syntax error at 'local_s_if_name' in configuration file\n",
            "Log: CU failed to start due to syntax error at 'local_s_if_name' in configuration file\n",
            "Related Config: local_s_if_name\n",
            "Notes: The 'local_s_if_name' parameter likely contains a syntax error. Common issues include:\n",
            "- Missing semicolon (`;`) at the end\n",
            "- Missing double quotes (`\"`) around interface name\n",
            "- Wrong indentation or being placed outside of NETWORK_INTERFACES block\n",
            "\n",
            "CU and DU on the same machine:\n",
            "- local_s_if_name = \"lo\";\n",
            "\n",
            "CU and DU on different machines:   \n",
            "- Actual NIC name, e.g., `\"eno1\"`, `\"eth0\"`, `\"ens3\"`\n",
            "- local_s_if_name = \"eno1\";\n",
            "- local_s_if_name = \"eth0\";\n",
            "- local_s_if_name = \"ens3\";\n",
            "Ensure the interface name is correctly specified and matches the actual network interface on your system.\n",
            "\n",
            "This parameter is used to specify the local interface name for CU-DU communication.\n",
            "Use \"lo\" if CU and DU are on the same machine, or a valid NIC like \"eno1\" if they are on separate hosts.\n",
            "default value: \"lo\"\n",
            "Metadata: {'notes': 'The \\'local_s_if_name\\' parameter likely contains a syntax error. Common issues include:\\n- Missing semicolon (`;`) at the end\\n- Missing double quotes (`\"`) around interface name\\n- Wrong indentation or being placed outside of NETWORK_INTERFACES block\\n\\nCU and DU on the same machine:\\n- local_s_if_name = \"lo\";\\n\\nCU and DU on different machines:   \\n- Actual NIC name, e.g., `\"eno1\"`, `\"eth0\"`, `\"ens3\"`\\n- local_s_if_name = \"eno1\";\\n- local_s_if_name = \"eth0\";\\n- local_s_if_name = \"ens3\";\\nEnsure the interface name is correctly specified and matches the actual network interface on your system.\\n\\nThis parameter is used to specify the local interface name for CU-DU communication.\\nUse \"lo\" if CU and DU are on the same machine, or a valid NIC like \"eno1\" if they are on separate hosts.\\ndefault value: \"lo\"', 'related_config': 'local_s_if_name', 'stage': 'cu_config_parsing', 'symptom': \"CU failed to start due to syntax error at 'local_s_if_name' in configuration file\"}\n"
          ]
        }
      ],
      "source": [
        "with open(debug_yaml_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    debug_data = yaml.safe_load(f)\n",
        "\n",
        "# The embedding format for each entry (based on symptom and log as the primary content)\n",
        "embedding_docs = []\n",
        "for item in debug_data:\n",
        "    related_config_str = \", \".join(item.get(\"related_config\", []))\n",
        "    notes = item.get(\"notes\", \"\")\n",
        "    \n",
        "    content = (\n",
        "        f\"Stage: {item['stage']}\\n\"\n",
        "        f\"Symptom: {item['symptom']}\\n\"\n",
        "        f\"Log: {item['log_snippet']}\\n\"\n",
        "        f\"Related Config: {related_config_str}\\n\"\n",
        "        f\"Notes: {notes}\"\n",
        "    )\n",
        "    \n",
        "    metadata = {\n",
        "        \"stage\": item[\"stage\"],\n",
        "        \"symptom\": item[\"symptom\"],\n",
        "        \"related_config\": related_config_str,\n",
        "        \"notes\": notes,\n",
        "    }\n",
        "\n",
        "    embedding_docs.append({\"content\": content, \"metadata\": metadata})\n",
        "\n",
        "# pprint.pprint(embedding_docs) #for checking\n",
        "\n",
        "# ä½ ä¹Ÿå¯ä»¥æ”¹ç”¨ Gemini æˆ– OpenAI embedding\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# å°‡æ–‡æœ¬åµŒå…¥å‘é‡ä¸¦å­˜å…¥ Chroma è³‡æ–™åº«\n",
        "texts = [d[\"content\"] for d in embedding_docs]\n",
        "metadatas = [d[\"metadata\"] for d in embedding_docs]\n",
        "\n",
        "vectordb = Chroma.from_texts(texts, embedding=embedding, metadatas=metadatas, persist_directory=\"./error_db\")\n",
        "vectordb.persist()\n",
        "\n",
        "print(\"âœ… Debug embedding å»ºç«‹å®Œæˆä¸¦å·²å„²å­˜\")\n",
        "\n",
        "\n",
        "\n",
        "# æª¢æŸ¥åµŒå…¥ç¸½ç­†æ•¸\n",
        "print(\"ğŸ“¦ ç¸½ç­†æ•¸ï¼š\", vectordb._collection.count())\n",
        "# é¡¯ç¤ºå‰å¹¾ç­†åµŒå…¥è³‡æ–™å…§å®¹ï¼ˆåŒ…æ‹¬åŸå§‹æ–‡æœ¬èˆ‡ metadataï¼‰\n",
        "peek_data = vectordb._collection.get(limit=1)\n",
        "\n",
        "for i in range(len(peek_data[\"documents\"])):\n",
        "    print(f\"\\n--- Entry {i+1} ---\")\n",
        "    print(\"Document ID:\", peek_data[\"ids\"][i])\n",
        "    print(\"Document Text:\", peek_data[\"documents\"][i])\n",
        "    print(\"Metadata:\", peek_data[\"metadatas\"][i])\n",
        "\n",
        "peek_data = vectordb._collection.get(limit=1, offset=vectordb._collection.count() - 1)\n",
        "\n",
        "# é¡¯ç¤ºè©²ç­†å…§å®¹\n",
        "for i in range(len(peek_data[\"documents\"])):\n",
        "    print(f\"\\n--- Last Entry ---\")\n",
        "    print(\"Document ID:\", peek_data[\"ids\"][i])\n",
        "    print(\"Document Text:\", peek_data[\"documents\"][i])\n",
        "    print(\"Metadata:\", peek_data[\"metadatas\"][i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check CU log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ›‘ Detected syntax error in config:\n",
            "ğŸ“„ File: /home/aiml/johnson/Scenario/Scenario_Latest_for_cu_testing/input_data/conf/68_cu_gnb_local_s_if_name.conf\n",
            "ğŸ“ Line 27: local_s_address = \"127.0.0.5\";\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "ğŸ§© Extracted config key: local_s_address\n",
            "ğŸ§  Constructed Semantic Query: CU failed to start due to `local_s_address` syntax error in configuration file\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "ğŸ” Checking document...\n",
            "ğŸ“„ Page Content: Stage: cu_init\n",
            "Symptom: CU failed to start because local_s_address could not be assigned or bound\n",
            "Lo...\n",
            "ğŸ“‘ related_config: ['local_s_address']\n",
            "------\n",
            "âœ… Matched related_config!\n",
            "\n",
            "--- âœ… Final Matched Case ---\n",
            "â— Symptom: CU failed to start because local_s_address could not be assigned or bound\n",
            "ğŸ”§ Related Config: local_s_address\n",
            "âŒ CU initialization failed: no success indicators found.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "CU failed to start due to `local_s_address` syntax error in configuration file| CU init failure\n"
          ]
        }
      ],
      "source": [
        "syntax_error_pattern = re.compile(r'\\[LIBCONFIG\\] file (?P<filepath>.+?) - line (?P<linenum>\\d+): syntax error')\n",
        "success_keywords = [\n",
        "    \"Send NGSetupRequest to AMF\",\n",
        "    \"Received NGSetupResponse from AMF\",\n",
        "    \"Starting F1AP at CU\"\n",
        "]\n",
        "\n",
        "matched_case = None\n",
        "found_by_syntax_error = False\n",
        "\n",
        "\n",
        "if not Path(cu_log_file).exists():\n",
        "    raise FileNotFoundError(f\"Log file not found: {cu_log_file}\")\n",
        "if not Path(debug_yaml_path).exists():\n",
        "    raise FileNotFoundError(f\"Debug YAML not found: {debug_yaml_path}\")\n",
        "\n",
        "# Read debug.yaml\n",
        "with open(debug_yaml_path, 'r', encoding='utf-8') as f:\n",
        "    debug_data = yaml.safe_load(f)\n",
        "\n",
        "# Organize (log_snippet, stage) pairs\n",
        "target_entries = []\n",
        "found_results = []\n",
        "found_set = set()\n",
        "all_lines = []\n",
        "query = \"\"\n",
        "\n",
        "for item in debug_data:\n",
        "    if 'log_snippet' in item:\n",
        "        raw_snippet = item['log_snippet']\n",
        "        stage = item.get('stage', 'unknown')\n",
        "\n",
        "        # å¦‚æœæ˜¯ listï¼ˆå¤šå€‹ snippetï¼‰\n",
        "        if isinstance(raw_snippet, list):\n",
        "            for s in raw_snippet:\n",
        "                target_entries.append((s.strip(), stage))\n",
        "        # å¦‚æœæ˜¯å­—ä¸²ä¸”åŒ…å«åˆ†è™Ÿï¼ˆå¤šæ®µ snippetï¼‰\n",
        "        elif isinstance(raw_snippet, str) and \";\" in raw_snippet:\n",
        "            parts = [s.strip() for s in raw_snippet.split(\";\")]\n",
        "            for p in parts:\n",
        "                target_entries.append((p, stage))\n",
        "        # å–®ä¸€å­—ä¸² snippet\n",
        "        elif isinstance(raw_snippet, str):\n",
        "            target_entries.append((raw_snippet.strip(), stage))\n",
        "\n",
        "\n",
        "assert_exit_found = False\n",
        "\n",
        "# æœç´¢ log\n",
        "with open(cu_log_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "    for raw_line in f:\n",
        "        line = clean_text(raw_line)\n",
        "        all_lines.append(line)  # æš«å­˜æ‰€æœ‰ log è¡Œï¼Œä¾›å¾ŒçºŒç¼ºå¤±åˆ¤æ–·ç”¨\n",
        "\n",
        "        # âœ… å¦‚æœåµæ¸¬åˆ° syntax error ä¸¦è¼¸å‡ºéŒ¯èª¤è¡Œ\n",
        "        match = syntax_error_pattern.search(line)\n",
        "        if match:\n",
        "            conf_path_str = match.group(\"filepath\")\n",
        "            line_num = int(match.group(\"linenum\"))\n",
        "            filename = Path(conf_path_str).name\n",
        "            conf_path = current_cu_config_path.parent / filename\n",
        "\n",
        "\n",
        "            if conf_path.exists():\n",
        "                try:\n",
        "                    with open(conf_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as cf:\n",
        "                        lines = cf.readlines()\n",
        "                        if 0 < line_num <= len(lines):\n",
        "                            error_line = lines[line_num - 1].strip()\n",
        "                        else:\n",
        "                            error_line = \"<Line number out of range>\"\n",
        "\n",
        "                        # === ğŸ” é¡¯ç¤ºéŒ¯èª¤è¡Œèˆ‡æª”æ¡ˆè³‡è¨Š ===\n",
        "                        print(\"\\nğŸ›‘ Detected syntax error in config:\")\n",
        "                        print(f\"ğŸ“„ File: {conf_path}\")\n",
        "                        print(f\"ğŸ“ Line {line_num}: {error_line}\")\n",
        "                        print(\"--------------------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "                        # === ğŸ§© æ“·å– config key ===\n",
        "                        config_key = extract_config_key(error_line)\n",
        "                        print(f\"ğŸ§© Extracted config key: {config_key}\")\n",
        "\n",
        "                        # === ğŸ§  å»ºç«‹èªæ„ queryï¼ˆå¯ä¾›èªæ„ fallback ä½¿ç”¨ï¼‰ ===\n",
        "                        query = f\"CU failed to start due to `{config_key}` syntax error in configuration file\"\n",
        "                        print(f\"ğŸ§  Constructed Semantic Query: {query}\")\n",
        "\n",
        "                        print(\"--------------------------------------------------------------------------------------------------------------\")\n",
        "                        # === ğŸ” æŸ¥è©¢ç›¸é—œé…ç½®æ¡ˆä¾‹ ===\n",
        "                        results = vectordb.similarity_search(config_key, k=1)\n",
        "                        matched_case = None\n",
        "                        for doc in results:\n",
        "                            rc_raw = doc.metadata.get('related_config', [])\n",
        "                            rc_list = rc_raw if isinstance(rc_raw, list) else [rc_raw]\n",
        "\n",
        "                            print(\"ğŸ” Checking document...\")\n",
        "                            print(f\"ğŸ“„ Page Content: {doc.page_content[:100]}...\")  # é¿å…è¼¸å‡ºå¤ªé•·\n",
        "                            print(f\"ğŸ“‘ related_config: {rc_list}\")\n",
        "                            print(\"------\")\n",
        "\n",
        "                            if config_key in rc_list:\n",
        "                                matched_case = doc\n",
        "                                found_by_syntax_error = True\n",
        "                                print(\"âœ… Matched related_config!\")\n",
        "                                \n",
        "                                break\n",
        "\n",
        "                        if matched_case:\n",
        "                            print(\"\\n--- âœ… Final Matched Case ---\")\n",
        "                            # print(f\"ğŸ“ Case: {matched_case.page_content}\")\n",
        "                            print(f\"â— Symptom: {matched_case.metadata.get('symptom', '')}\")\n",
        "                            print(f\"ğŸ”§ Related Config: {matched_case.metadata.get('related_config', '')}\")\n",
        "                            # print(f\"ğŸ“Œ Notes:\\n{matched_case.metadata.get('notes', '')}\")\n",
        "                        else:\n",
        "                            print(f\"âŒ No case matched for related_config = {config_key}\")\n",
        "\n",
        "\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    print(f\"âš ï¸ Failed to read config file {conf_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"âŒ Config file not found: {conf_path}\")\n",
        "            continue  # å¯ä»¥é¸æ“‡æ˜¯å¦ç¹¼çºŒè™•ç†å…¶ä»– log è¡Œ\n",
        "        \n",
        "        # âœ… å¦‚æœåµæ¸¬åˆ° Assert_Exit_ ä¸¦è¼¸å‡ºéŒ¯èª¤è¡Œ\n",
        "        if 'Assert_Exit_' in line:\n",
        "            print(f\"âš ï¸ Found Assert_Exit_ log, skipping critical log check.\")\n",
        "            break\n",
        "\n",
        "        # âœ… æ¯”å° debug.yaml ä¸­çš„ snippet\n",
        "        for snippet, stage in target_entries:\n",
        "            snippet_cleaned = clean_text(snippet)\n",
        "            if snippet_cleaned in line:\n",
        "                found_results.append((stage, snippet))\n",
        "                found_set.add(snippet_cleaned)\n",
        "          \n",
        "# âœ… Stage 1: All keywords matched â†’ CU initialization is fully successful\n",
        "if all(any(kw in line for line in all_lines) for kw in success_keywords):\n",
        "    print(\"âœ… CU initialization successful (all indicators present).\")\n",
        "    print(\"ğŸŸ¢ No configuration issue detected, no correction needed.\")\n",
        "    query += \" CU initialization success\"\n",
        "\n",
        "# ğŸŸ¡ Stage 2: Some keywords matched â†’ Possible segmentation fault suspected\n",
        "elif any(any(kw in line for line in all_lines) for kw in success_keywords):\n",
        "    matched_keywords = [kw for kw in success_keywords if any(kw in line for line in all_lines)]\n",
        "    print(\"ğŸŸ¡ CU initialization incomplete: possible segmentation fault.\")\n",
        "    print(f\"ğŸ” Detected partial success indicators: {matched_keywords}\")\n",
        "    if not any(\"Starting F1AP at CU\" in line for line in all_lines):\n",
        "        print(\"F1AP not started â€” possible invalid tr_s_preference setting\")\n",
        "        query += \" CU F1AP not started â€” possible invalid tr_s_preference setting\"\n",
        "\n",
        "# âŒ Stage 3: No success keywords found â†’ CU initialization failed\n",
        "else:\n",
        "    print(\"âŒ CU initialization failed: no success indicators found.\")\n",
        "    query += \"| CU init failure\"\n",
        "\n",
        "print(\"-------------------------------------------------------------------------------------------------------------\")\n",
        "print(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Matched Case ---\n",
            "Symptom: CU failed to start because local_s_address could not be assigned or bound\n",
            "Related Config: local_s_address\n",
            "Notes:,The configured local_s_address is not available on this machine or is already in use.\n",
            "Make sure the IP address exists on the host, is not already in use, and is properly configured in the network interface.\n",
            "All related errors above are caused by this root issue.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if found_by_syntax_error:\n",
        "    if matched_case:\n",
        "        matched_content = matched_case.page_content\n",
        "        matched_symptom = matched_case.metadata.get(\"symptom\", \"\")\n",
        "        matched_related_config = matched_case.metadata.get(\"related_config\", \"\")\n",
        "        matched_Notes = matched_case.metadata.get(\"notes\", \"\")\n",
        "\n",
        "        # è¼¸å‡ºçµæœ\n",
        "        print(\"\\n--- Matched Case ---\")\n",
        "        # print(f\"Case: {matched_content}\")\n",
        "        # print(\"==============\")\n",
        "        print(f\"Symptom: {matched_symptom}\")\n",
        "        print(f\"Related Config: {matched_related_config}\")\n",
        "        print(f\"Notes:,{matched_Notes}\")\n",
        "    else:\n",
        "        print(\"âŒ No case matched for related_config = {config_key}\")\n",
        "    \n",
        "else:\n",
        "    results = vectordb.similarity_search(query, k=2)\n",
        "    matched_case = results[0]\n",
        "\n",
        "    # æ­£ç¢ºå­˜å– dict ä¸­çš„å€¼\n",
        "\n",
        "    matched_content = matched_case.page_content\n",
        "    matched_symptom = matched_case.metadata.get(\"symptom\", \"\")\n",
        "    matched_related_config = matched_case.metadata.get(\"related_config\", \"\")\n",
        "    matched_Notes = matched_case.metadata.get(\"notes\", \"\")\n",
        "\n",
        "    # è¼¸å‡ºçµæœ\n",
        "    print(\"\\n--- Matched Case ---\")\n",
        "    # print(f\"Case: {matched_content}\")\n",
        "    # print(\"==============\")\n",
        "    print(f\"Symptom: {matched_symptom}\")\n",
        "    print(f\"Related Config: {matched_related_config}\")\n",
        "    print(f\"Notes:,{matched_Notes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(current_cu_config_json_path, \"r\") as f:\n",
        "    config_cu_segments_context = json.load(f)\n",
        "# with open(current_du_config_json_path, \"r\") as f:\n",
        "#     config_du_segments_context = json.load(f)\n",
        "# with open(current_ru_config_json_path, \"r\") as f:\n",
        "#     config_ru_segments_context = json.load(f)\n",
        "\n",
        "\n",
        "with open(reference_context_path, \"r\") as f:\n",
        "    reference_context = f.read()\n",
        "\n",
        "# RAG prompt_template\n",
        "rag_prompt_template = f\"\"\"\n",
        "You are a 5G network expert. Your job is to revise configuration files based on observed network issues and debug knowledge.\n",
        "\n",
        "Issue Description:\n",
        "\"{query}\"\n",
        "\n",
        "Matching debug knowledge:\n",
        "{matched_case.metadata[\"symptom\"]}\n",
        "{matched_case.metadata[\"notes\"] if matched_case.metadata[\"notes\"] else \"\"}\n",
        "Relevant parameters: {matched_case.metadata[\"related_config\"]}\n",
        "\n",
        "\n",
        "\n",
        "Reference Device Address Table (external reference file):\n",
        "{reference_context}\n",
        "\n",
        "Current CU configuration block:\n",
        "{config_cu_segments_context}\n",
        "\n",
        "\n",
        "Please revise the configuration using correct addresses from the reference. Output only the revised config section.\n",
        "\n",
        "Return a list of JSON objects with the following structure:\n",
        "\n",
        "```json\n",
        "[\n",
        "  {{\n",
        "    \"label\": \"parameter_name\",\n",
        "    \"content\": \"parameter_name = (...);\",\n",
        "    \"reference_reason\": \"Explain the value based on reference_context (e.g., matches expected IP or MAC). If no relevant info is found, respond with: 'No relevant information found in reference_context.'\",    \n",
        "    \"model_reason\": \"Additional expert analysis in 1-2 sentences explaining why this change is necessary, beneficial, or resolves a network issue.\"\n",
        "    \"target\": \"CU\" or \"DU\" or \"RU\" or \"FH\"\n",
        "  }},\n",
        "  ...\n",
        "]\n",
        "```\n",
        "\n",
        "- Only include parameters listed in 'Relevant parameters'.\n",
        "- Do not include any explanation outside of the JSON structure.\n",
        "- Keep \"reference_reason\" based on the reference table.\n",
        "- Derive \"model_reason\" from your own technical reasoning.\n",
        "- Set the \"target\" field based on the location of the parameter: \"CU\" for CU config, \"DU\" for DU config, \"RU\" for RU config, and \"FH\" for FH config.\n",
        "- If multiple configuration problems exist at the same time, return multiple JSON objects â€” one for each necessary change.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "none_rag_prompt_template = f\"\"\"\n",
        "You are a 5G network expert. Your job is to revise configuration files based on observed network issues.\n",
        "\n",
        "Issue Description:\n",
        "\"{query}\"\n",
        "\n",
        "Current configuration block:\n",
        "{config_cu_segments_context}\n",
        "\n",
        "Please revise the configuration to resolve the described issue based on your technical expertise.\n",
        "\n",
        "Return a list of JSON objects with the following structure:\n",
        "\n",
        "```json\n",
        "[\n",
        "  {{\n",
        "    \"label\": \"parameter_name\",\n",
        "    \"content\": \"parameter_name = (...);\",\n",
        "    \"model_reason\": \"Technical explanation in 1-2 sentences explaining why this change is necessary, beneficial, or resolves the network issue.\"\n",
        "  }},\n",
        "  ...\n",
        "]\n",
        "```\n",
        "\n",
        "- Only revise parameters that are necessary to resolve the issue.\n",
        "- If no changes are needed, return an empty list: []\n",
        "- Strictly output only valid JSON without any additional text or explanation.\n",
        "\"\"\"\n",
        "\n",
        "reason_output_dir = \"Reason\"\n",
        "os.makedirs(reason_output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "def save_json(filename, data):\n",
        "    with open(os.path.join(reason_output_dir, filename), \"w\", encoding='utf-8') as f:\n",
        "        json.dump(data, f, indent=2, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM Suggested Revisionsï¼š\n",
            "\n",
            "```json\n",
            "[\n",
            "  {\n",
            "    \"label\": \"local_s_address\",\n",
            "    \"content\": \"local_s_address = \\\"192.168.8.43\\\";\",\n",
            "    \"reference_reason\": \"The CU gNB IP address is 192.168.8.43 based on the provided config block values for GNB_IPV4_ADDRESS_FOR_NG_AMF and GNB_IPV4_ADDRESS_FOR_NGU.\",\n",
            "    \"model_reason\": \"The CU init failure indicates that the configured local_s_address (127.0.0.5) is either unavailable or already in use on the host. Assigning the CU's gNB IP address to local_s_address ensures the CU can bind to a valid and available address for F1AP communication.\",\n",
            "    \"target\": \"CU\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "========= Suggestions =========\n",
            "[{'label': 'local_s_address', 'content': 'local_s_address = \"192.168.8.43\";', 'reference_reason': 'The CU gNB IP address is 192.168.8.43 based on the provided config block values for GNB_IPV4_ADDRESS_FOR_NG_AMF and GNB_IPV4_ADDRESS_FOR_NGU.', 'model_reason': \"The CU init failure indicates that the configured local_s_address (127.0.0.5) is either unavailable or already in use on the host. Assigning the CU's gNB IP address to local_s_address ensures the CU can bind to a valid and available address for F1AP communication.\", 'target': 'CU'}]\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "skip_processing = False\n",
        "\n",
        "if query.strip().lower() == \"cu initialization success\":\n",
        "    print(\"âœ… No inference needed: CU initialization is successful.\")\n",
        "\n",
        "    cu_suggestions = [\n",
        "        {\n",
        "            \"label\": \"CU Initialization\",\n",
        "            \"reference_reason\": \"\"\n",
        "        }\n",
        "    ]\n",
        "    change_log = [\n",
        "        (\"CU Initialization\", None, None, \"CU started successfully. No revision needed.\")\n",
        "    ]\n",
        "\n",
        "    dummy_content = \"// No changes needed. CU started successfully.\\n\"\n",
        "\n",
        "    # ç”¢å‡ºåŸå§‹ä¿®æ”¹çµæœèˆ‡ SFT æª”æ¡ˆ\n",
        "    save_modified_config(dummy_content, rag_after_cu_conf_path, \"CU\")\n",
        "    compare_conf_files(current_cu_config_path, rag_after_cu_conf_path, diff_log_path)\n",
        "    save_sft_data(change_log, rag_after_cu_conf_path, \"CU\", cu_suggestions, current_cu_config_path)\n",
        "\n",
        "    # â• é‡æ–°å‘½åç‚º pass_ é–‹é ­çš„æª”æ¡ˆ\n",
        "    base_dir = os.path.dirname(rag_after_cu_conf_path)\n",
        "    base_name = os.path.basename(rag_after_cu_conf_path)\n",
        "    sft_name = base_name.replace(\".conf\", \"_CU_sft.json\")  # æ ¹æ“šä½ ç¿’æ…£çš„å‘½åæ–¹å¼\n",
        "    sft_path = os.path.join(base_dir, sft_name)\n",
        "\n",
        "    if os.path.exists(sft_path):\n",
        "        new_sft_path = os.path.join(base_dir, f\"pass_{sft_name}\")\n",
        "        os.rename(sft_path, new_sft_path)\n",
        "        print(f\"âœ… Renamed SFT file to: {new_sft_path}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ SFT file not found for renaming: {sft_path}\")\n",
        "\n",
        "    du_suggestions = []\n",
        "    skip_processing = True\n",
        "\n",
        "\n",
        "# else:\n",
        "#     client = OpenAI(\n",
        "#       base_url = \"https://integrate.api.nvidia.com/v1\",\n",
        "#       api_key = \"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\"\n",
        "#     )\n",
        "\n",
        "#     response = client.chat.completions.create(\n",
        "#         model=\"meta/llama-3.3-70b-instruct\",\n",
        "#         messages=[\n",
        "#             {\"role\": \"user\", \"content\": rag_prompt_template}\n",
        "#         ],\n",
        "#         temperature=0.2,\n",
        "#         top_p=0.7,\n",
        "#         max_tokens=1024,\n",
        "#         stream=False\n",
        "#     )\n",
        "#     rag_response_text = response.choices[0].message.content\n",
        "#     print(\"LLM Suggested Revisionsï¼š\\n\")\n",
        "#     print(\"RAG Response:\\n\", rag_response_text)\n",
        "\n",
        "\n",
        "#     rag_llm_suggestions = parse_llm_response(rag_response_text)\n",
        "#     print(\"========= Suggestions =========\")\n",
        "#     print(rag_llm_suggestions)\n",
        "\n",
        "\n",
        "\n",
        "#     save_json(f\"{matched_related_config}_rag.json\", rag_llm_suggestions)\n",
        "#     # save_json(f\"{matched_related_config}_none_rag.json\", none_rag_llm_suggestions)\n",
        "\n",
        "else:\n",
        "    genai.configure(api_key=\"AIzaSyBqqRkGvIkAJUZ5MgYcvxw4t3Lx12D4rWU\") #set your API key here\n",
        "    model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "    rag_response = model.generate_content(rag_prompt_template)                                # Gemini API\n",
        "    # LLM Suggested Revisions\n",
        "    print(\"LLM Suggested Revisionsï¼š\\n\")\n",
        "    print(rag_response.text)\n",
        "\n",
        "    rag_llm_suggestions = parse_llm_response(rag_response.text)\n",
        "    print(\"========= Suggestions =========\")\n",
        "    print(rag_llm_suggestions)\n",
        "\n",
        "\n",
        "\n",
        "    save_json(f\"{matched_related_config}_rag.json\", rag_llm_suggestions)\n",
        "    # save_json(f\"{matched_related_config}_none_rag.json\", none_rag_llm_suggestions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# genai.configure(api_key=\"AIzaSyBqqRkGvIkAJUZ5MgYcvxw4t3Lx12D4rWU\") #set your API key here\n",
        "# model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# rag_response = model.generate_content(rag_prompt_template)                                # Gemini API\n",
        "# # LLM Suggested Revisions\n",
        "# print(\"LLM Suggested Revisionsï¼š\\n\")\n",
        "# print(rag_response.text)\n",
        "\n",
        "# rag_llm_suggestions = parse_llm_response(rag_response.text)\n",
        "# print(\"========= Suggestions =========\")\n",
        "# print(rag_llm_suggestions)\n",
        "\n",
        "\n",
        "\n",
        "# save_json(f\"{matched_related_config}_rag.json\", rag_llm_suggestions)\n",
        "# # save_json(f\"{matched_related_config}_none_rag.json\", none_rag_llm_suggestions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CU Suggestions:\n",
            "[Output error suppressed]\n",
            "DU Suggestions:\n",
            "[Output error suppressed]\n",
            "âœ… [CU] Updated file: /home/aiml/johnson/Scenario/Scenario_Latest_for_cu_testing/output_data/68_cu_gnb_local_s_if_name_modification.conf\n",
            "ğŸ› ï¸ [CU] Modified parameters:\n",
            " - local_s_address\n",
            "âœ… [CU] Updated file: /home/aiml/johnson/Scenario/Scenario_Latest_for_cu_testing/output_data/68_cu_gnb_local_s_if_name_modification.conf\n",
            "âœ… Diff saved to: /home/aiml/johnson/Scenario/Scenario_Latest_for_cu_testing/output_data/68_cu_gnb_local_s_if_name_diff.log\n",
            "\n",
            "ğŸ“ [CU] SFT data saved to: /home/aiml/johnson/Scenario/Scenario_Latest_for_cu_testing/output_data/68_cu_gnb_local_s_if_name_modification_CU_sft.json\n"
          ]
        }
      ],
      "source": [
        "if not skip_processing:\n",
        "    cu_suggestions, du_suggestions = split_suggestions_by_target(rag_llm_suggestions)\n",
        "\n",
        "    safe_print(\"CU Suggestions:\")\n",
        "    safe_print(cu_suggestions)\n",
        "    safe_print(\"DU Suggestions:\")\n",
        "    safe_print(du_suggestions)\n",
        "\n",
        "    process_config_type(\n",
        "        config_type=\"CU\",\n",
        "        suggestions=cu_suggestions,\n",
        "        current_path=current_cu_config_path,\n",
        "        modified_path=rag_after_cu_conf_path,\n",
        "        diff_log_path=diff_log_path\n",
        "    )\n",
        "else:\n",
        "    print(\"âœ… Skipped config processing due to no needed revisions.\")\n",
        "\n",
        "# process_config_type(\n",
        "#     config_type=\"DU\",\n",
        "#     suggestions=du_suggestions,\n",
        "#     current_path=current_du_config_path,\n",
        "#     modified_path=rag_after_du_conf_path,\n",
        "#     diff_log_path=\"/home/aiml/johnson/Scenario/Scenario_For_testing/DU/conf/du_diff_log.txt\"\n",
        "# )\n",
        "\n",
        "# process_config_type(\n",
        "#     config_type=\"RU\",\n",
        "#     suggestions=du_suggestions,\n",
        "#     current_path=current_ru_config_path,\n",
        "#     modified_path=rag_after_ru_conf_path,\n",
        "#     diff_log_path=\"/home/aiml/johnson/Scenario/Scenario_For_testing/DU/conf/du_diff_log.txt\"\n",
        "# )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
